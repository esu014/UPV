{"cells":[{"cell_type":"markdown","metadata":{"id":"fXgAhyAdgyvS"},"source":["# Perceptrón aplicado a iris"]},{"cell_type":"markdown","metadata":{"id":"qMuI1k_AgyvT"},"source":["**Lectura del corpus:** $\\;$ comprobamos también que las matrices de datos y etiquetas tienen las filas y columnas que toca"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KqKpXRcFgyvU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701785629639,"user_tz":-60,"elapsed":283,"user":{"displayName":"Pablo Raga","userId":"07948866859055017034"}},"outputId":"a89a0d3d-83c3-4743-ca49-286843f909e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["(150, 4) (150, 1) \n"," [[5.1015625  3.5        1.40039062 0.19995117 0.        ]\n"," [4.8984375  3.         1.40039062 0.19995117 0.        ]\n"," [4.69921875 3.19921875 1.29980469 0.19995117 0.        ]\n"," [4.6015625  3.09960938 1.5        0.19995117 0.        ]\n"," [5.         3.59960938 1.40039062 0.19995117 0.        ]]\n"]}],"source":["import numpy as np; from sklearn.datasets import load_iris\n","iris = load_iris(); X = iris.data.astype(np.float16);\n","y = iris.target.astype(np.uint).reshape(-1, 1);\n","print(X.shape, y.shape, \"\\n\", np.hstack([X, y])[:5, :])"]},{"cell_type":"markdown","metadata":{"id":"bNj7PTU5gyvU"},"source":["**Partición del corpus:** $\\;$ Creamos un split de iris con un $20\\%$ de datos para test y el resto para entrenamiento (training), barajando previamente los datos de acuerdo con una semilla dada para la generación de números aleatorios. Aquí, como en todo código que incluya aleatoriedad (que requiera generar números aleatorios), conviene fijar dicha semilla para poder reproducir experimentos con exactitud."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d3JVbmXUgyvV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701786559062,"user_tz":-60,"elapsed":296,"user":{"displayName":"Pablo Raga","userId":"07948866859055017034"}},"outputId":"b33d77d5-7254-4c93-8fee-a0d4bdfa8c9f"},"outputs":[{"output_type":"stream","name":"stdout","text":["(120, 4) (30, 4)\n"]}],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)\n","print(X_train.shape, X_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"Qy73P_ssgyvV"},"source":["**Implementación de Perceptrón:** $\\;$ devuelve pesos en notación homogénea, $\\mathbf{W}\\in\\mathbb{R}^{(1+D)\\times C};\\;$ también el número de errores e iteraciones ejecutadas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WIzbuFofgyvV"},"outputs":[],"source":["def perceptron(X, y, b=0.1, a=1.0, K=200):\n","    N, D = X.shape; Y = np.unique(y); C = Y.size; W = np.zeros((1+D, C))\n","    for k in range(1, K+1):\n","        E = 0\n","        for n in range(N):\n","            xn = np.array([1, *X[n, :]])\n","            cn = np.squeeze(np.where(Y==y[n]))\n","            gn = W[:,cn].T @ xn; err = False\n","            for c in np.arange(C):\n","                if c != cn and W[:,c].T @ xn + b >= gn:\n","                    W[:, c] = W[:, c] - a*xn; err = True\n","            if err:\n","                W[:, cn] = W[:, cn] + a*xn; E = E + 1\n","        if E == 0:\n","            break;\n","    return W, E, k"]},{"cell_type":"markdown","metadata":{"id":"y31EDfj7gyvW"},"source":["**Aprendizaje de un clasificador (lineal) con Perceptrón:** $\\;$ Perceptrón minimiza el número de errores de entrenamiento (con margen)\n","$$\\mathbf{W}^*=\\operatorname*{argmin}_{\\mathbf{W}=(\\boldsymbol{w}_1,\\dotsc,\\boldsymbol{w}_C)}\\sum_n\\;\\mathbb{Y}\\biggl(\\max_{c\\neq y_n}\\;\\boldsymbol{w}_c^t\\boldsymbol{x}_n+b \\;>\\; \\boldsymbol{w}_{y_n}^t\\boldsymbol{x}_n\\biggr)$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iAuEF10bgyvX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701786022654,"user_tz":-60,"elapsed":682,"user":{"displayName":"Pablo Raga","userId":"07948866859055017034"}},"outputId":"ff339e2f-4070-4583-c857-ebb08c5ea8de"},"outputs":[{"output_type":"stream","name":"stdout","text":["Número de iteraciones ejecutadas:  200\n","Número de errores de entrenamiento:  2\n","Vectores de pesos de las clases (en columnas y en notación homogénea):\n"," [[  10.           85.         -142.        ]\n"," [ -49.421875    -68.19140625 -176.47265625]\n"," [  50.171875     -1.72460938 -181.06445312]\n"," [-189.91210938  -87.70507812   68.69726562]\n"," [ -86.40258789 -137.78149414  157.88415527]]\n"]}],"source":["W, E, k = perceptron(X_train, y_train)\n","print(\"Número de iteraciones ejecutadas: \", k)\n","print(\"Número de errores de entrenamiento: \", E)\n","print(\"Vectores de pesos de las clases (en columnas y en notación homogénea):\\n\", W);"]},{"cell_type":"markdown","metadata":{"id":"u7G-a5_zgyvX"},"source":["**Cálculo de la tasa de error en test:**\n","El siguiente conjunto de instrucciones calcula la tasa de error para un Perceptron entrenado con b=0.1 y a=1.0 (factor de aprendizaje o 'a') y K=200 iteraciones"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1W2qNWQggyvY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701786032695,"user_tz":-60,"elapsed":293,"user":{"displayName":"Pablo Raga","userId":"07948866859055017034"}},"outputId":"d1e9b02d-895a-42d3-aea4-6d4c02ad7aa2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tasa de error en test: 16.7%\n"]}],"source":["X_testh = np.hstack([np.ones((len(X_test), 1)), X_test])\n","y_test_pred  = np.argmax(X_testh @ W, axis=1).reshape(-1, 1)\n","err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n","print(f\"Tasa de error en test: {err_test:.1%}\")"]},{"cell_type":"code","source":["err_test * len(X_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qH2dVT9odSoK","executionInfo":{"status":"ok","timestamp":1701787828407,"user_tz":-60,"elapsed":284,"user":{"displayName":"Pablo Raga","userId":"07948866859055017034"}},"outputId":"f9ef52be-c706-48df-c426-b28ff67adb32"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5.0"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"43TAexM_gyvY"},"source":["**Ajuste del margen:** $\\;$ el siguiente bucle es un experimento que ejecuta 5 veces el algoritmo del Perceptron con 5 valores diferentes de $b$ y valor por defecto $α$=1.0 y K=1000 iteraciones, mostrando el error de entrenamiento para cada valor de $b$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5EokibkXgyvY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701786906283,"user_tz":-60,"elapsed":16991,"user":{"displayName":"Pablo Raga","userId":"07948866859055017034"}},"outputId":"49d4b56f-f99e-485f-9753-102d7baa7f74"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.0 3 1000\n","0.01 5 1000\n","0.1 3 1000\n","10 6 1000\n","100 6 1000\n"]}],"source":["for b in (.0, .01, .1, 10, 100):\n","    W, E, k = perceptron(X_train, y_train, b=b, K=1000)\n","    print(b, E, k)"]},{"cell_type":"markdown","metadata":{"id":"faP8KC55gyvY"},"source":["**Interpretación de resultados:** $\\;$ los datos de entrenamiento no parecen linealmente separables; no está claro que un margen mayor que cero pueda mejorar resultados, sobre todo porque solo tenemos $30$ muestras de test; con margen nulo ya hemos visto que se obtiene un error (en test) del $16.7\\%$\n","-"]},{"cell_type":"code","source":["print('# a b K E k Ete')\n","for a in (0.1,1.0,10,100,1000,10000):\n","  for b in (0.0,0.01,0.1,1.0,100,1000):\n","    for k in (200,500,800,1000):\n","      W, E, k = perceptron(X_train, y_train, b=b, a=a, K=k)\n","      X_testh = np.hstack([np.ones((len(X_test), 1)), X_test])\n","      y_test_pred  = np.argmax(X_testh @ W, axis=1).reshape(-1, 1)\n","      err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n","      print('%8.2f %8.2f %6d %6d %6d %8.3f' %(a, b, k, E,k,err_test))\n","  print('#------- ------ ---- ---- ---- ----')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pUssUWqheyU3","executionInfo":{"status":"ok","timestamp":1701789850336,"user_tz":-60,"elapsed":302657,"user":{"displayName":"Pablo Raga","userId":"07948866859055017034"}},"outputId":"ccd8677a-e19e-41ee-d74f-5c37f06abd3f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["# a b K E k Ete\n","    0.10     0.00    200      6    200    0.133\n","    0.10     0.00    500      1    500    0.167\n","    0.10     0.00    800      5    800    0.133\n","    0.10     0.00   1000      3   1000    0.033\n","    0.10     0.01    200      2    200    0.167\n","    0.10     0.01    500      3    500    0.033\n","    0.10     0.01    800      5    800    0.133\n","    0.10     0.01   1000      3   1000    0.133\n","    0.10     0.10    200      8    200    0.100\n","    0.10     0.10    500      5    500    0.167\n","    0.10     0.10    800      3    800    0.167\n","    0.10     0.10   1000      4   1000    0.033\n","    0.10     1.00    200     10    200    0.033\n","    0.10     1.00    500      3    500    0.033\n","    0.10     1.00    800      6    800    0.067\n","    0.10     1.00   1000      6   1000    0.067\n","    0.10   100.00    200     29    200    0.000\n","    0.10   100.00    500     19    500    0.033\n","    0.10   100.00    800     15    800    0.033\n","    0.10   100.00   1000     15   1000    0.033\n","    0.10  1000.00    200     82    200    0.133\n","    0.10  1000.00    500     66    500    0.000\n","    0.10  1000.00    800     47    800    0.000\n","    0.10  1000.00   1000     41   1000    0.000\n","#------- ------ ---- ---- ---- ----\n","    1.00     0.00    200      6    200    0.133\n","    1.00     0.00    500      1    500    0.167\n","    1.00     0.00    800      5    800    0.133\n","    1.00     0.00   1000      3   1000    0.033\n","    1.00     0.01    200      6    200    0.133\n","    1.00     0.01    500      5    500    0.167\n","    1.00     0.01    800      6    800    0.133\n","    1.00     0.01   1000      5   1000    0.133\n","    1.00     0.10    200      2    200    0.167\n","    1.00     0.10    500      3    500    0.033\n","    1.00     0.10    800      5    800    0.133\n","    1.00     0.10   1000      3   1000    0.133\n","    1.00     1.00    200      8    200    0.100\n","    1.00     1.00    500      5    500    0.167\n","    1.00     1.00    800      3    800    0.167\n","    1.00     1.00   1000      4   1000    0.033\n","    1.00   100.00    200     13    200    0.100\n","    1.00   100.00    500     10    500    0.033\n","    1.00   100.00    800      6    800    0.033\n","    1.00   100.00   1000      6   1000    0.000\n","    1.00  1000.00    200     29    200    0.000\n","    1.00  1000.00    500     19    500    0.033\n","    1.00  1000.00    800     15    800    0.033\n","    1.00  1000.00   1000     15   1000    0.033\n","#------- ------ ---- ---- ---- ----\n","   10.00     0.00    200      6    200    0.133\n","   10.00     0.00    500      1    500    0.167\n","   10.00     0.00    800      5    800    0.133\n","   10.00     0.00   1000      3   1000    0.033\n","   10.00     0.01    200      6    200    0.133\n","   10.00     0.01    500      5    500    0.167\n","   10.00     0.01    800      6    800    0.133\n","   10.00     0.01   1000      3   1000    0.167\n","   10.00     0.10    200      6    200    0.133\n","   10.00     0.10    500      5    500    0.167\n","   10.00     0.10    800      6    800    0.133\n","   10.00     0.10   1000      5   1000    0.133\n","   10.00     1.00    200      2    200    0.167\n","   10.00     1.00    500      3    500    0.033\n","   10.00     1.00    800      5    800    0.133\n","   10.00     1.00   1000      3   1000    0.133\n","   10.00   100.00    200     10    200    0.033\n","   10.00   100.00    500      3    500    0.033\n","   10.00   100.00    800      6    800    0.067\n","   10.00   100.00   1000      6   1000    0.067\n","   10.00  1000.00    200     13    200    0.100\n","   10.00  1000.00    500     10    500    0.033\n","   10.00  1000.00    800      6    800    0.033\n","   10.00  1000.00   1000      6   1000    0.000\n","#------- ------ ---- ---- ---- ----\n","  100.00     0.00    200      6    200    0.133\n","  100.00     0.00    500      1    500    0.167\n","  100.00     0.00    800      5    800    0.133\n","  100.00     0.00   1000      3   1000    0.033\n","  100.00     0.01    200      6    200    0.133\n","  100.00     0.01    500      1    500    0.167\n","  100.00     0.01    800      5    800    0.133\n","  100.00     0.01   1000      3   1000    0.033\n","  100.00     0.10    200      6    200    0.133\n","  100.00     0.10    500      5    500    0.167\n","  100.00     0.10    800      6    800    0.133\n","  100.00     0.10   1000      3   1000    0.167\n","  100.00     1.00    200      6    200    0.133\n","  100.00     1.00    500      5    500    0.167\n","  100.00     1.00    800      6    800    0.133\n","  100.00     1.00   1000      5   1000    0.133\n","  100.00   100.00    200      8    200    0.100\n","  100.00   100.00    500      5    500    0.167\n","  100.00   100.00    800      3    800    0.167\n","  100.00   100.00   1000      4   1000    0.033\n","  100.00  1000.00    200     10    200    0.033\n","  100.00  1000.00    500      3    500    0.033\n","  100.00  1000.00    800      6    800    0.067\n","  100.00  1000.00   1000      6   1000    0.067\n","#------- ------ ---- ---- ---- ----\n"," 1000.00     0.00    200      6    200    0.133\n"," 1000.00     0.00    500      1    500    0.167\n"," 1000.00     0.00    800      5    800    0.133\n"," 1000.00     0.00   1000      3   1000    0.033\n"," 1000.00     0.01    200      6    200    0.133\n"," 1000.00     0.01    500      1    500    0.167\n"," 1000.00     0.01    800      5    800    0.133\n"," 1000.00     0.01   1000      3   1000    0.033\n"," 1000.00     0.10    200      6    200    0.133\n"," 1000.00     0.10    500      1    500    0.167\n"," 1000.00     0.10    800      5    800    0.133\n"," 1000.00     0.10   1000      3   1000    0.033\n"," 1000.00     1.00    200      6    200    0.133\n"," 1000.00     1.00    500      5    500    0.167\n"," 1000.00     1.00    800      6    800    0.133\n"," 1000.00     1.00   1000      3   1000    0.167\n"," 1000.00   100.00    200      2    200    0.167\n"," 1000.00   100.00    500      3    500    0.033\n"," 1000.00   100.00    800      5    800    0.133\n"," 1000.00   100.00   1000      3   1000    0.133\n"," 1000.00  1000.00    200      8    200    0.100\n"," 1000.00  1000.00    500      5    500    0.167\n"," 1000.00  1000.00    800      3    800    0.167\n"," 1000.00  1000.00   1000      4   1000    0.033\n","#------- ------ ---- ---- ---- ----\n","10000.00     0.00    200      6    200    0.133\n","10000.00     0.00    500      1    500    0.167\n","10000.00     0.00    800      5    800    0.133\n","10000.00     0.00   1000      3   1000    0.033\n","10000.00     0.01    200      6    200    0.133\n","10000.00     0.01    500      1    500    0.167\n","10000.00     0.01    800      5    800    0.133\n","10000.00     0.01   1000      3   1000    0.033\n","10000.00     0.10    200      6    200    0.133\n","10000.00     0.10    500      1    500    0.167\n","10000.00     0.10    800      5    800    0.133\n","10000.00     0.10   1000      3   1000    0.033\n","10000.00     1.00    200      6    200    0.133\n","10000.00     1.00    500      1    500    0.167\n","10000.00     1.00    800      5    800    0.133\n","10000.00     1.00   1000      3   1000    0.033\n","10000.00   100.00    200      6    200    0.133\n","10000.00   100.00    500      5    500    0.167\n","10000.00   100.00    800      6    800    0.133\n","10000.00   100.00   1000      5   1000    0.133\n","10000.00  1000.00    200      2    200    0.167\n","10000.00  1000.00    500      3    500    0.033\n","10000.00  1000.00    800      5    800    0.133\n","10000.00  1000.00   1000      3   1000    0.133\n","#------- ------ ---- ---- ---- ----\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"zRISKlhJirep"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Ejercicio para realizar en clase:**\n","---\n","\n","\n","Escribe el código python necesario para mostrar resultados del error de entrenamiento (E), número de iteraciones empleadas (k) y error de test (err_test) para combinaciones de diferentes valores de:\n","---\n","\n","# *   a: (factor de aprendizaje: utiliza valores 0.1, 1.0, 10, 100, 1000, 10000)\n","# *   b: (margen: utiliza valores 0.0, 0.01, 0.1, 1.0, 100, 1000)\n","# *   K: (iteraciones: utiliza valores 200, 500, 800 y 1000)\n","\n","\n","Utiliza la siguientes instrucciones para mostrar la cabecera:\n","---\n","\n","`print('#      a        b      K      E      k      Ete');`\n","---\n","`print('#-------   ------   ----   ----   ----     ----');`\n","---\n","\n","y la siguiente instrucción para mostrar los resultados:\n","\n","`print('%8.2f %8.2f %6d %6d %6d %8.3f' %(a, b, K, E,k,err_test))`\n","---\n","\n","\n","\n"],"metadata":{"id":"5wMTWdG3xWUH"}},{"cell_type":"markdown","source":["Después de completar la ejecución del bucle, podemos extraer algunas conclusiones. En primer lugar, a simple vista, se puede concluir que el error de prueba disminuye a medida que el factor de aprendizaje disminuye. Esto se evidencia al observar que el error de prueba de 0.00 solo se encuentra para los factores de aprendizaje 0.1, 1.0 y 10. Además, también podemos notar que estos errores tienden a ocurrir en los márgenes más altos y en las iteraciones más elevadas. En resumen, para obtener el menor error de prueba, la combinación ideal sería un factor de aprendizaje mínimo (0.1), un margen máximo (1000) y la iteración más grande (1000)."],"metadata":{"id":"nICZ4Yly-Jpd"}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"orig_nbformat":4,"colab":{"provenance":[{"file_id":"1AV1U-pJ69beaexdgZYS_ZzcKZUekXws7","timestamp":1701790003656}]}},"nbformat":4,"nbformat_minor":0}