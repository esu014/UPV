{"cells":[{"cell_type":"markdown","metadata":{"id":"fXgAhyAdgyvS"},"source":["# Perceptrón aplicado a iris"]},{"cell_type":"markdown","metadata":{"id":"qMuI1k_AgyvT"},"source":["**Lectura del corpus:** $\\;$ comprobamos también que las matrices de datos y etiquetas tienen las filas y columnas que toca"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KqKpXRcFgyvU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"abe7fc38-0f79-427a-cc7f-d296b1c5e9df"},"outputs":[{"output_type":"stream","name":"stdout","text":["(1797, 64) (1797, 1) \n"," [[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n","  15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n","   0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n","   0.  0.  0.  0.  6. 13. 10.  0.  0.  0.  0.]\n"," [ 0.  0.  0. 12. 13.  5.  0.  0.  0.  0.  0. 11. 16.  9.  0.  0.  0.  0.\n","   3. 15. 16.  6.  0.  0.  0.  7. 15. 16. 16.  2.  0.  0.  0.  0.  1. 16.\n","  16.  3.  0.  0.  0.  0.  1. 16. 16.  6.  0.  0.  0.  0.  1. 16. 16.  6.\n","   0.  0.  0.  0.  0. 11. 16. 10.  0.  0.  1.]\n"," [ 0.  0.  0.  4. 15. 12.  0.  0.  0.  0.  3. 16. 15. 14.  0.  0.  0.  0.\n","   8. 13.  8. 16.  0.  0.  0.  0.  1.  6. 15. 11.  0.  0.  0.  1.  8. 13.\n","  15.  1.  0.  0.  0.  9. 16. 16.  5.  0.  0.  0.  0.  3. 13. 16. 16. 11.\n","   5.  0.  0.  0.  0.  3. 11. 16.  9.  0.  2.]\n"," [ 0.  0.  7. 15. 13.  1.  0.  0.  0.  8. 13.  6. 15.  4.  0.  0.  0.  2.\n","   1. 13. 13.  0.  0.  0.  0.  0.  2. 15. 11.  1.  0.  0.  0.  0.  0.  1.\n","  12. 12.  1.  0.  0.  0.  0.  0.  1. 10.  8.  0.  0.  0.  8.  4.  5. 14.\n","   9.  0.  0.  0.  7. 13. 13.  9.  0.  0.  3.]\n"," [ 0.  0.  0.  1. 11.  0.  0.  0.  0.  0.  0.  7.  8.  0.  0.  0.  0.  0.\n","   1. 13.  6.  2.  2.  0.  0.  0.  7. 15.  0.  9.  8.  0.  0.  5. 16. 10.\n","   0. 16.  6.  0.  0.  4. 15. 16. 13. 16.  1.  0.  0.  0.  0.  3. 15. 10.\n","   0.  0.  0.  0.  0.  2. 16.  4.  0.  0.  4.]]\n"]}],"source":["import numpy as np; from sklearn.datasets import load_digits\n","iris = load_digits(); X = iris.data.astype(np.float16);\n","y = iris.target.astype(np.uint).reshape(-1, 1);\n","print(X.shape, y.shape, \"\\n\", np.hstack([X, y])[:5, :])"]},{"cell_type":"markdown","metadata":{"id":"bNj7PTU5gyvU"},"source":["**Partición del corpus:** $\\;$ Creamos un split de iris con un $20\\%$ de datos para test y el resto para entrenamiento (training), barajando previamente los datos de acuerdo con una semilla dada para la generación de números aleatorios. Aquí, como en todo código que incluya aleatoriedad (que requiera generar números aleatorios), conviene fijar dicha semilla para poder reproducir experimentos con exactitud."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d3JVbmXUgyvV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"91a899bd-4638-4765-ac2c-ca7f41fb4bef"},"outputs":[{"output_type":"stream","name":"stdout","text":["(1437, 64) (360, 64)\n"]}],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)\n","print(X_train.shape, X_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"Qy73P_ssgyvV"},"source":["**Implementación de Perceptrón:** $\\;$ devuelve pesos en notación homogénea, $\\mathbf{W}\\in\\mathbb{R}^{(1+D)\\times C};\\;$ también el número de errores e iteraciones ejecutadas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WIzbuFofgyvV"},"outputs":[],"source":["def perceptron(X, y, b=0.1, a=1.0, K=200):\n","    N, D = X.shape; Y = np.unique(y); C = Y.size; W = np.zeros((1+D, C))\n","    for k in range(1, K+1):\n","        E = 0\n","        for n in range(N):\n","            xn = np.array([1, *X[n, :]])\n","            cn = np.squeeze(np.where(Y==y[n]))\n","            gn = W[:,cn].T @ xn; err = False\n","            for c in np.arange(C):\n","                if c != cn and W[:,c].T @ xn + b >= gn:\n","                    W[:, c] = W[:, c] - a*xn; err = True\n","            if err:\n","                W[:, cn] = W[:, cn] + a*xn; E = E + 1\n","        if E == 0:\n","            break;\n","    return W, E, k"]},{"cell_type":"markdown","metadata":{"id":"y31EDfj7gyvW"},"source":["**Aprendizaje de un clasificador (lineal) con Perceptrón:** $\\;$ Perceptrón minimiza el número de errores de entrenamiento (con margen)\n","$$\\mathbf{W}^*=\\operatorname*{argmin}_{\\mathbf{W}=(\\boldsymbol{w}_1,\\dotsc,\\boldsymbol{w}_C)}\\sum_n\\;\\mathbb{Y}\\biggl(\\max_{c\\neq y_n}\\;\\boldsymbol{w}_c^t\\boldsymbol{x}_n+b \\;>\\; \\boldsymbol{w}_{y_n}^t\\boldsymbol{x}_n\\biggr)$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iAuEF10bgyvX","colab":{"base_uri":"https://localhost:8080/"},"outputId":"33b3e3a1-072b-4d1a-8702-d8676185f4ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Número de iteraciones ejecutadas:  106\n","Número de errores de entrenamiento:  0\n","Vectores de pesos de las clases (en columnas y en notación homogénea):\n"," [[-1.420e+02 -2.120e+02 -1.420e+02 -1.430e+02 -1.040e+02 -1.480e+02\n","  -1.390e+02 -1.410e+02 -1.450e+02 -1.910e+02]\n"," [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n","   0.000e+00  0.000e+00  0.000e+00  0.000e+00]\n"," [-1.700e+01 -2.600e+01  3.300e+01  5.000e+01 -3.500e+01 -1.300e+01\n","  -4.200e+01  4.200e+01 -5.700e+01 -1.720e+02]\n"," [-8.910e+02 -1.029e+03 -9.100e+02 -9.130e+02 -8.400e+02 -5.020e+02\n","  -1.030e+03 -8.650e+02 -8.160e+02 -9.250e+02]\n"," [-1.639e+03 -1.522e+03 -1.635e+03 -1.680e+03 -2.119e+03 -1.819e+03\n","  -1.746e+03 -1.539e+03 -1.805e+03 -1.457e+03]\n"," [-1.708e+03 -2.382e+03 -1.691e+03 -1.321e+03 -1.808e+03 -1.673e+03\n","  -1.757e+03 -1.597e+03 -1.682e+03 -1.599e+03]\n"," [-1.084e+03 -4.850e+02 -9.800e+02 -9.290e+02 -1.280e+03 -6.760e+02\n","  -1.110e+03 -8.550e+02 -1.067e+03 -1.035e+03]\n"," [-3.240e+02 -2.790e+02 -2.640e+02 -2.130e+02 -3.290e+02  1.420e+02\n","  -1.300e+02  0.000e+00 -4.330e+02 -4.500e+01]\n"," [-3.200e+01 -2.000e+01 -1.400e+01 -3.500e+01  9.000e+01 -4.600e+01\n","  -1.500e+01  2.700e+01 -5.700e+01 -6.500e+01]\n"," [ 0.000e+00 -2.000e+00 -1.400e+01 -1.000e+01  0.000e+00  4.000e+00\n","   0.000e+00  0.000e+00  1.600e+01  0.000e+00]\n"," [-3.260e+02 -5.750e+02 -7.400e+01 -2.160e+02 -2.350e+02 -3.760e+02\n","  -4.530e+02 -1.920e+02 -2.570e+02 -2.350e+02]\n"," [-1.814e+03 -2.179e+03 -1.755e+03 -1.546e+03 -1.944e+03 -1.621e+03\n","  -1.901e+03 -1.557e+03 -1.720e+03 -1.684e+03]\n"," [-1.701e+03 -2.024e+03 -2.052e+03 -1.740e+03 -2.012e+03 -1.862e+03\n","  -1.815e+03 -1.690e+03 -1.749e+03 -1.859e+03]\n"," [-1.730e+03 -1.614e+03 -1.514e+03 -1.547e+03 -2.071e+03 -1.739e+03\n","  -2.005e+03 -1.333e+03 -1.995e+03 -1.829e+03]\n"," [-1.363e+03 -1.564e+03 -1.445e+03 -1.341e+03 -1.508e+03 -1.467e+03\n","  -1.377e+03 -1.475e+03 -1.363e+03 -1.452e+03]\n"," [-3.790e+02 -5.790e+02 -3.240e+02 -1.250e+02 -4.110e+02 -3.120e+02\n","  -2.630e+02 -3.770e+02 -1.710e+02 -2.400e+02]\n"," [-3.000e+01 -1.700e+01 -1.300e+01 -3.300e+01  7.600e+01 -2.600e+01\n","  -2.000e+00  3.400e+01 -1.400e+01 -6.700e+01]\n"," [ 0.000e+00  0.000e+00 -6.000e+00 -6.000e+00  0.000e+00  0.000e+00\n","   0.000e+00  0.000e+00  9.000e+00  0.000e+00]\n"," [-3.920e+02 -2.750e+02 -5.250e+02 -4.570e+02 -3.780e+02 -5.120e+02\n","  -5.500e+02 -7.320e+02 -3.050e+02 -3.940e+02]\n"," [-1.667e+03 -1.791e+03 -1.808e+03 -2.076e+03 -1.614e+03 -1.654e+03\n","  -1.597e+03 -1.980e+03 -1.629e+03 -1.711e+03]\n"," [-1.472e+03 -1.055e+03 -1.710e+03 -1.821e+03 -1.356e+03 -1.429e+03\n","  -1.419e+03 -1.669e+03 -1.489e+03 -1.425e+03]\n"," [-1.767e+03 -1.204e+03 -1.275e+03 -1.515e+03 -1.553e+03 -1.961e+03\n","  -1.762e+03 -1.491e+03 -1.585e+03 -1.393e+03]\n"," [-1.162e+03 -1.474e+03 -1.378e+03 -1.573e+03 -1.208e+03 -1.616e+03\n","  -1.603e+03 -1.270e+03 -1.238e+03 -8.920e+02]\n"," [-2.050e+02 -8.400e+01  4.800e+01 -2.130e+02 -2.150e+02 -7.680e+02\n","  -3.040e+02 -1.460e+02 -2.290e+02 -2.260e+02]\n"," [-3.000e+00 -7.000e+00  0.000e+00 -1.000e+01  5.400e+01 -1.100e+01\n","  -2.000e+00 -3.000e+00 -3.000e+00 -3.200e+01]\n"," [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n","   0.000e+00  0.000e+00  0.000e+00  0.000e+00]\n"," [-2.000e+02 -4.870e+02 -4.870e+02 -5.400e+02 -3.550e+02 -2.060e+02\n","  -8.400e+01 -4.540e+02 -2.400e+02 -3.720e+02]\n"," [-1.400e+03 -1.460e+03 -1.863e+03 -1.782e+03 -1.172e+03 -1.222e+03\n","  -1.377e+03 -1.440e+03 -1.610e+03 -1.293e+03]\n"," [-1.791e+03 -1.758e+03 -2.103e+03 -1.986e+03 -1.605e+03 -1.743e+03\n","  -1.661e+03 -1.992e+03 -1.529e+03 -1.485e+03]\n"," [-1.964e+03 -1.494e+03 -1.858e+03 -1.469e+03 -1.568e+03 -1.481e+03\n","  -1.791e+03 -1.564e+03 -1.698e+03 -1.727e+03]\n"," [-1.125e+03 -8.970e+02 -1.156e+03 -1.372e+03 -1.276e+03 -1.184e+03\n","  -1.096e+03 -1.066e+03 -1.065e+03 -8.930e+02]\n"," [-1.400e+02 -2.640e+02 -1.390e+02 -4.890e+02  2.500e+01 -4.740e+02\n","  -3.720e+02  5.400e+01 -1.790e+02 -1.470e+02]\n"," [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  6.000e+00 -1.000e+00\n","  -1.000e+00 -3.000e+00 -1.000e+00 -3.000e+00]\n"," [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n","   0.000e+00  0.000e+00  0.000e+00  0.000e+00]\n"," [-4.200e+01 -1.970e+02 -2.840e+02 -1.820e+02  7.000e+00 -9.100e+01\n","   5.500e+01 -1.060e+02 -3.380e+02 -5.430e+02]\n"," [-8.840e+02 -7.500e+02 -1.178e+03 -1.057e+03 -7.980e+02 -7.990e+02\n","  -9.230e+02 -8.320e+02 -1.042e+03 -9.690e+02]\n"," [-1.642e+03 -1.590e+03 -1.592e+03 -1.441e+03 -1.543e+03 -1.683e+03\n","  -1.302e+03 -1.553e+03 -1.180e+03 -1.327e+03]\n"," [-1.802e+03 -1.560e+03 -1.555e+03 -1.479e+03 -1.401e+03 -1.842e+03\n","  -1.591e+03 -1.465e+03 -1.562e+03 -1.684e+03]\n"," [-1.242e+03 -1.110e+03 -1.365e+03 -1.396e+03 -9.910e+02 -1.214e+03\n","  -1.348e+03 -1.032e+03 -1.459e+03 -1.246e+03]\n"," [-2.370e+02 -4.490e+02 -4.360e+02 -1.000e+02  1.240e+02 -1.270e+02\n","   3.000e+00 -5.800e+01 -6.250e+02 -4.180e+02]\n"," [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n","   0.000e+00  0.000e+00  0.000e+00  0.000e+00]\n"," [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n","   0.000e+00  0.000e+00  0.000e+00  0.000e+00]\n"," [-2.010e+02 -3.450e+02 -1.500e+01 -1.950e+02  3.400e+02 -2.400e+02\n","  -3.420e+02 -1.290e+02 -1.750e+02 -1.970e+02]\n"," [-7.450e+02 -1.026e+03 -9.230e+02 -1.136e+03 -8.790e+02 -1.248e+03\n","  -5.680e+02 -9.620e+02 -6.100e+02 -1.321e+03]\n"," [-1.026e+03 -7.440e+02 -7.410e+02 -1.502e+03 -6.430e+02 -1.178e+03\n","  -9.530e+02 -1.061e+03 -1.059e+03 -1.685e+03]\n"," [-1.292e+03 -1.319e+03 -1.665e+03 -1.087e+03 -9.670e+02 -1.324e+03\n","  -1.066e+03 -1.206e+03 -9.740e+02 -1.639e+03]\n"," [-1.173e+03 -1.388e+03 -1.532e+03 -1.057e+03 -1.105e+03 -1.363e+03\n","  -1.218e+03 -1.414e+03 -1.148e+03 -1.592e+03]\n"," [-3.760e+02 -7.310e+02 -4.770e+02 -7.500e+01 -1.330e+02 -3.270e+02\n","  -2.220e+02 -3.360e+02 -1.860e+02 -7.040e+02]\n"," [ 0.000e+00 -2.000e+00  4.000e+00 -4.000e+00  0.000e+00  0.000e+00\n","   4.000e+00  0.000e+00 -2.000e+00  0.000e+00]\n"," [ 0.000e+00 -3.000e+00  3.000e+00  0.000e+00 -3.000e+00  0.000e+00\n","   0.000e+00  0.000e+00 -3.000e+00  0.000e+00]\n"," [-2.120e+02 -4.700e+01  4.100e+01 -1.680e+02  8.000e+01  3.700e+01\n","  -2.080e+02 -1.560e+02 -1.590e+02 -4.700e+01]\n"," [-1.014e+03 -1.273e+03 -1.151e+03 -1.221e+03 -1.440e+03 -1.263e+03\n","  -1.040e+03 -1.252e+03 -9.960e+02 -1.314e+03]\n"," [-1.400e+03 -1.303e+03 -9.770e+02 -1.624e+03 -1.398e+03 -1.603e+03\n","  -1.399e+03 -1.363e+03 -1.634e+03 -1.575e+03]\n"," [-1.388e+03 -1.096e+03 -1.073e+03 -1.412e+03 -1.407e+03 -1.350e+03\n","  -1.518e+03 -1.549e+03 -1.708e+03 -1.407e+03]\n"," [-1.303e+03 -1.368e+03 -1.169e+03 -1.215e+03 -1.674e+03 -1.314e+03\n","  -1.208e+03 -1.736e+03 -1.322e+03 -1.348e+03]\n"," [-4.420e+02 -6.290e+02 -2.890e+02 -9.800e+01 -5.860e+02 -6.660e+02\n","  -3.170e+02 -4.880e+02 -2.470e+02 -3.440e+02]\n"," [-2.200e+01  9.700e+01  6.000e+00 -1.120e+02 -9.000e+00 -1.500e+01\n","  -8.900e+01 -4.000e+00 -3.000e+01  6.100e+01]\n"," [ 0.000e+00 -1.000e+00  1.000e+00  0.000e+00 -1.000e+00  0.000e+00\n","   0.000e+00  0.000e+00 -1.000e+00  0.000e+00]\n"," [-1.700e+01 -1.000e+00  1.040e+02 -2.100e+01 -8.100e+01  3.700e+01\n","  -4.000e+01 -1.000e+01 -5.400e+01 -1.120e+02]\n"," [-8.820e+02 -1.102e+03 -7.280e+02 -6.160e+02 -8.080e+02 -5.290e+02\n","  -9.310e+02 -9.030e+02 -1.224e+03 -8.450e+02]\n"," [-1.500e+03 -1.599e+03 -1.605e+03 -1.583e+03 -1.861e+03 -1.450e+03\n","  -1.812e+03 -1.864e+03 -1.716e+03 -1.565e+03]\n"," [-1.746e+03 -1.750e+03 -1.612e+03 -1.754e+03 -1.723e+03 -1.673e+03\n","  -1.716e+03 -1.793e+03 -1.454e+03 -1.672e+03]\n"," [-1.090e+03 -1.048e+03 -9.650e+02 -1.082e+03 -1.321e+03 -9.950e+02\n","  -1.021e+03 -1.305e+03 -1.267e+03 -1.053e+03]\n"," [-1.950e+02 -4.300e+01  8.200e+01 -1.580e+02 -3.160e+02 -4.380e+02\n","  -2.200e+02 -3.670e+02 -4.180e+02 -1.730e+02]\n"," [-4.200e+01  1.600e+02  1.090e+02 -1.230e+02 -9.000e+00 -6.000e+01\n","  -1.320e+02 -1.000e+01 -1.440e+02 -3.500e+01]]\n"]}],"source":["W, E, k = perceptron(X_train, y_train)\n","print(\"Número de iteraciones ejecutadas: \", k)\n","print(\"Número de errores de entrenamiento: \", E)\n","print(\"Vectores de pesos de las clases (en columnas y en notación homogénea):\\n\", W);"]},{"cell_type":"markdown","metadata":{"id":"u7G-a5_zgyvX"},"source":["**Cálculo de la tasa de error en test:**\n","El siguiente conjunto de instrucciones calcula la tasa de error para un Perceptron entrenado con b=0.1 y a=1.0 (factor de aprendizaje o 'a') y K=200 iteraciones"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1W2qNWQggyvY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f6ed7039-7fdb-4170-9939-786fcd0affd4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tasa de error en test: 4.4%\n"]}],"source":["X_testh = np.hstack([np.ones((len(X_test), 1)), X_test])\n","y_test_pred  = np.argmax(X_testh @ W, axis=1).reshape(-1, 1)\n","err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n","print(f\"Tasa de error en test: {err_test:.1%}\")"]},{"cell_type":"code","source":["err_test * len(X_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qH2dVT9odSoK","outputId":"bed954b9-b428-414e-960b-6976acae7204"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["16.0"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"43TAexM_gyvY"},"source":["**Ajuste del margen:** $\\;$ el siguiente bucle es un experimento que ejecuta 5 veces el algoritmo del Perceptron con 5 valores diferentes de $b$ y valor por defecto $α$=1.0 y K=1000 iteraciones, mostrando el error de entrenamiento para cada valor de $b$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5EokibkXgyvY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"83399517-0987-4d52-d6b4-843f81386c90"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.0 0 106\n","0.01 0 106\n","0.1 0 106\n","10 0 140\n","100 0 97\n"]}],"source":["for b in (.0, .01, .1, 10, 100):\n","    W, E, k = perceptron(X_train, y_train, b=b, K=1000)\n","    print(b, E, k)"]},{"cell_type":"markdown","metadata":{"id":"faP8KC55gyvY"},"source":["**Interpretación de resultados:** $\\;$ los datos de entrenamiento no parecen linealmente separables; no está claro que un margen mayor que cero pueda mejorar resultados, sobre todo porque solo tenemos $30$ muestras de test; con margen nulo ya hemos visto que se obtiene un error (en test) del $16.7\\%$\n","-"]},{"cell_type":"code","source":["min_error = float('inf')\n","best_combination = None\n","print('# a b K E k Ete')\n","for a in (0.1,1.0,10,100,1000,10000):\n","  for b in (0.0,0.01,0.1,1.0,100,1000):\n","    for K in (200,500,800,1000):\n","      W, E, k = perceptron(X_train, y_train, b, a, K)\n","      X_testh = np.hstack([np.ones((len(X_test), 1)), X_test])\n","      y_test_pred  = np.argmax(X_testh @ W, axis=1).reshape(-1, 1)\n","      err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n","      print('%8.2f %8.2f %6d %6d %6d %8.3f' %(a, b, K, E,k,err_test))\n","      if err_test < min_error:\n","                min_error = err_test\n","                best_combination = (a, b, k)\n","\n","  print('#------- ------ ---- ---- ---- ----')\n","print('Mejor combinación con menor error:', best_combination)\n","print('Menor error encontrado:', min_error)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pUssUWqheyU3","outputId":"89ba749f-369d-4ede-c72d-3db6753158bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["# a b K E k Ete\n","    0.10     0.00    200      0    112    0.061\n","    0.10     0.00    500      0    112    0.061\n","    0.10     0.00    800      0    112    0.061\n","    0.10     0.00   1000      0    112    0.061\n","    0.10     0.01    200      0    106    0.044\n","    0.10     0.01    500      0    106    0.044\n","    0.10     0.01    800      0    106    0.044\n","    0.10     0.01   1000      0    106    0.044\n","    0.10     0.10    200      0     88    0.042\n","    0.10     0.10    500      0     88    0.042\n","    0.10     0.10    800      0     88    0.042\n","    0.10     0.10   1000      0     88    0.042\n","    0.10     1.00    200      0     88    0.042\n","    0.10     1.00    500      0     88    0.042\n","    0.10     1.00    800      0     88    0.042\n","    0.10     1.00   1000      0     88    0.042\n","    0.10   100.00    200      0    172    0.044\n","    0.10   100.00    500      0    172    0.044\n","    0.10   100.00    800      0    172    0.044\n","    0.10   100.00   1000      0    172    0.044\n","    0.10  1000.00    200     22    200    0.047\n","    0.10  1000.00    500      0    407    0.044\n","    0.10  1000.00    800      0    407    0.044\n","    0.10  1000.00   1000      0    407    0.044\n","#------- ------ ---- ---- ---- ----\n","    1.00     0.00    200      0    106    0.044\n","    1.00     0.00    500      0    106    0.044\n","    1.00     0.00    800      0    106    0.044\n","    1.00     0.00   1000      0    106    0.044\n","    1.00     0.01    200      0    106    0.044\n","    1.00     0.01    500      0    106    0.044\n","    1.00     0.01    800      0    106    0.044\n","    1.00     0.01   1000      0    106    0.044\n","    1.00     0.10    200      0    106    0.044\n","    1.00     0.10    500      0    106    0.044\n","    1.00     0.10    800      0    106    0.044\n","    1.00     0.10   1000      0    106    0.044\n","    1.00     1.00    200      0     88    0.042\n","    1.00     1.00    500      0     88    0.042\n","    1.00     1.00    800      0     88    0.042\n","    1.00     1.00   1000      0     88    0.042\n","    1.00   100.00    200      0     97    0.042\n","    1.00   100.00    500      0     97    0.042\n","    1.00   100.00    800      0     97    0.042\n","    1.00   100.00   1000      0     97    0.042\n","    1.00  1000.00    200      0    133    0.042\n","    1.00  1000.00    500      0    133    0.042\n","    1.00  1000.00    800      0    133    0.042\n","    1.00  1000.00   1000      0    133    0.042\n","#------- ------ ---- ---- ---- ----\n","   10.00     0.00    200      0    106    0.044\n","   10.00     0.00    500      0    106    0.044\n","   10.00     0.00    800      0    106    0.044\n","   10.00     0.00   1000      0    106    0.044\n","   10.00     0.01    200      0    106    0.044\n","   10.00     0.01    500      0    106    0.044\n","   10.00     0.01    800      0    106    0.044\n","   10.00     0.01   1000      0    106    0.044\n","   10.00     0.10    200      0    106    0.044\n","   10.00     0.10    500      0    106    0.044\n","   10.00     0.10    800      0    106    0.044\n","   10.00     0.10   1000      0    106    0.044\n","   10.00     1.00    200      0    106    0.044\n","   10.00     1.00    500      0    106    0.044\n","   10.00     1.00    800      0    106    0.044\n","   10.00     1.00   1000      0    106    0.044\n","   10.00   100.00    200      0    140    0.050\n","   10.00   100.00    500      0    140    0.050\n","   10.00   100.00    800      0    140    0.050\n","   10.00   100.00   1000      0    140    0.050\n","   10.00  1000.00    200      0     97    0.042\n","   10.00  1000.00    500      0     97    0.042\n","   10.00  1000.00    800      0     97    0.042\n","   10.00  1000.00   1000      0     97    0.042\n","#------- ------ ---- ---- ---- ----\n","  100.00     0.00    200      0    106    0.044\n","  100.00     0.00    500      0    106    0.044\n","  100.00     0.00    800      0    106    0.044\n","  100.00     0.00   1000      0    106    0.044\n","  100.00     0.01    200      0    106    0.044\n","  100.00     0.01    500      0    106    0.044\n","  100.00     0.01    800      0    106    0.044\n","  100.00     0.01   1000      0    106    0.044\n","  100.00     0.10    200      0    106    0.044\n","  100.00     0.10    500      0    106    0.044\n","  100.00     0.10    800      0    106    0.044\n","  100.00     0.10   1000      0    106    0.044\n","  100.00     1.00    200      0    106    0.044\n","  100.00     1.00    500      0    106    0.044\n","  100.00     1.00    800      0    106    0.044\n","  100.00     1.00   1000      0    106    0.044\n","  100.00   100.00    200      0     88    0.042\n","  100.00   100.00    500      0     88    0.042\n","  100.00   100.00    800      0     88    0.042\n","  100.00   100.00   1000      0     88    0.042\n","  100.00  1000.00    200      0    140    0.050\n","  100.00  1000.00    500      0    140    0.050\n","  100.00  1000.00    800      0    140    0.050\n","  100.00  1000.00   1000      0    140    0.050\n","#------- ------ ---- ---- ---- ----\n"," 1000.00     0.00    200      0    106    0.044\n"," 1000.00     0.00    500      0    106    0.044\n"," 1000.00     0.00    800      0    106    0.044\n"," 1000.00     0.00   1000      0    106    0.044\n"," 1000.00     0.01    200      0    106    0.044\n"," 1000.00     0.01    500      0    106    0.044\n"," 1000.00     0.01    800      0    106    0.044\n"," 1000.00     0.01   1000      0    106    0.044\n"," 1000.00     0.10    200      0    106    0.044\n"," 1000.00     0.10    500      0    106    0.044\n"," 1000.00     0.10    800      0    106    0.044\n"," 1000.00     0.10   1000      0    106    0.044\n"," 1000.00     1.00    200      0    106    0.044\n"," 1000.00     1.00    500      0    106    0.044\n"," 1000.00     1.00    800      0    106    0.044\n"," 1000.00     1.00   1000      0    106    0.044\n"," 1000.00   100.00    200      0    106    0.044\n"," 1000.00   100.00    500      0    106    0.044\n"," 1000.00   100.00    800      0    106    0.044\n"," 1000.00   100.00   1000      0    106    0.044\n"," 1000.00  1000.00    200      0     88    0.042\n"," 1000.00  1000.00    500      0     88    0.042\n"," 1000.00  1000.00    800      0     88    0.042\n"," 1000.00  1000.00   1000      0     88    0.042\n","#------- ------ ---- ---- ---- ----\n","10000.00     0.00    200      0    106    0.044\n","10000.00     0.00    500      0    106    0.044\n","10000.00     0.00    800      0    106    0.044\n","10000.00     0.00   1000      0    106    0.044\n","10000.00     0.01    200      0    106    0.044\n","10000.00     0.01    500      0    106    0.044\n","10000.00     0.01    800      0    106    0.044\n","10000.00     0.01   1000      0    106    0.044\n","10000.00     0.10    200      0    106    0.044\n","10000.00     0.10    500      0    106    0.044\n","10000.00     0.10    800      0    106    0.044\n","10000.00     0.10   1000      0    106    0.044\n","10000.00     1.00    200      0    106    0.044\n","10000.00     1.00    500      0    106    0.044\n","10000.00     1.00    800      0    106    0.044\n","10000.00     1.00   1000      0    106    0.044\n","10000.00   100.00    200      0    106    0.044\n","10000.00   100.00    500      0    106    0.044\n","10000.00   100.00    800      0    106    0.044\n","10000.00   100.00   1000      0    106    0.044\n","10000.00  1000.00    200      0    106    0.044\n","10000.00  1000.00    500      0    106    0.044\n","10000.00  1000.00    800      0    106    0.044\n","10000.00  1000.00   1000      0    106    0.044\n","#------- ------ ---- ---- ---- ----\n","Mejor combinación con menor error: (0.1, 0.1, 88)\n","Menor error encontrado: 0.041666666666666664\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"zRISKlhJirep"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Ejercicio para realizar en clase:**\n","---\n","\n","\n","Escribe el código python necesario para mostrar resultados del error de entrenamiento (E), número de iteraciones empleadas (k) y error de test (err_test) para combinaciones de diferentes valores de:\n","---\n","\n","# *   a: (factor de aprendizaje: utiliza valores 0.1, 1.0, 10, 100, 1000, 10000)\n","# *   b: (margen: utiliza valores 0.0, 0.01, 0.1, 1.0, 100, 1000)\n","# *   K: (iteraciones: utiliza valores 200, 500, 800 y 1000)\n","\n","\n","Utiliza la siguientes instrucciones para mostrar la cabecera:\n","---\n","\n","`print('#      a        b      K      E      k      Ete');`\n","---\n","`print('#-------   ------   ----   ----   ----     ----');`\n","---\n","\n","y la siguiente instrucción para mostrar los resultados:\n","\n","`print('%8.2f %8.2f %6d %6d %6d %8.3f' %(a, b, K, E,k,err_test))`\n","---\n","\n","\n","\n"],"metadata":{"id":"5wMTWdG3xWUH"}},{"cell_type":"markdown","source":["Después de completar la ejecución del bucle, podemos derivar conclusiones valiosas de los resultados obtenidos.\n","\n","A primera vista, se nota que en general, todos los valores presentan errores de prueba muy similares entre sí. Sin embargo, al profundizar en los resultados, se identifican ciertos patrones significativos. Por ejemplo, los errores de prueba más altos tienden a asociarse con márgenes más bajos, especialmente evidente con el factor de aprendizaje establecido en 0.1. Esta tendencia se mantiene hasta el factor de aprendizaje 10000, donde deja de cumplirse. También es notable que a menor error de entrenamiento corresponde, en general, un menor error de prueba. Asimismo, valores más bajos de factor de aprendizaje suelen coincidir con errores de prueba más bajos, y para un mayor número de iteraciones, se tiende a observar errores de prueba más bajos.\n","\n","En resumen, la combinación que parece proporcionar el menor error de prueba es un factor de aprendizaje de 0.1, un margen de 0.1 y 1000 iteraciones. Aunque la diferencia es prácticamente imperceptible y se expresa en centésimas, esta configuración se destaca como la más efectiva según los resultados obtenidos."],"metadata":{"id":"sX2_LQw9lfsf"}},{"cell_type":"markdown","source":[],"metadata":{"id":"15mwt5kQAJNJ"}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}